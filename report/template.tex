%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 11 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper


\usepackage{hyperref}
\usepackage{url}
\usepackage[pdftex]{graphicx}
\usepackage{amsfonts}
\usepackage{subfigure} 
\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{algcompatible}
\usepackage{framed}
\usepackage{balance}


\pdfminorversion=4

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command
\overrideIEEEmargins                                      % Needed to meet printer requirements.


\title{\LARGE \bf
RL Transfer Learning
}


\author{Mahmudur Rahman, Sums Uz Zaman}


\begin{document}

\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}

adsasd adasdasd  adsasd a dasd asd asd asdasdas dasdas das asdasdasd adsasd adasdasd  adsasd a dasd asd asd asdasdas dasdas das asdasdasd adsasd adasdasd  adsasd a dasd asd asd asdasdas dasdas das asdasdasd adsasd adasdasd  adsasd a dasd asd asd asdasdas dasdas das asdasdasd adsasd adasdasd  adsasd a dasd asd asd asdasdas dasdas das asdasdasd adsasd adasdasd  adsasd a dasd asd asd asdasdas dasdas das asdasdasd adsasd adasdasd  adsasd a dasd asd asd asdasdas dasdas das asdasdasd adsasd adasdasd  adsasd a dasd asd asd asdasdas dasdas das asdasdasd

\end{abstract}


\section{Introduction}

Humans and some other animales are good at varieties of tasks and can learn a new task very easily. The main catalyst of human as well as other animal's fast learning capability is  their use of of previous knowledge to learn a new task. They can incorporate all the relevant information in the context of the new task from all of their previous experiences. Transferring skills from previously learned tasks to the new tasks or in the new environments is also important in the context of Reinforcement Learning (RL)\cite{sutton1998introduction} agents. If a Reinforcement Learning (RL) agent can use the previously learned skills to a new task efficiently, it would learn the new task faster than learning from the scratch. The main challenge is to enable the agent to use the previous experiences efficiently with the change of the task and the type of the environment. In this project, we used a common feature embedding method to transfer skills from the source task to the target task in order to learn the target task faster. 
\linebreak
\par Recent advancement of Deep Reinforcement Learning\cite{mnih2013playing} opens a new horizon of using Reinforcement Learning algorithm with diverse set of applications. Deep reinforcement learning achieves human level performance on some complex task. It also allows to use different representation of both the environment and action spaces. However, as deep learning agents require a large amount of data and the agent have to collect the data by the interaction with the environment, it takes a long time for a deep reinforcement learning agent to learn a particular task from scratch. This creates an important bottleneck of using deep reinforcement learning agents in relatively complex environments. Training deep reinforcement learning agent for relatively complex task can be made faster by transferring skills from previously learned similar task with similar environment. However, as deep learning model acts like blackbox, it is difficult to transfer neural network policy to the target task even with sightly changed environment. 
\linebreak

\par If the source and the target environment have some common state features, these features can be used to transfer skill from a source task to a target task.  But those common features may not be available to the to the policy during the training of the agent. If the policy learns about those feature during the interaction with the environment, the learned policy can use those skills to the new task with the new environment. Our proposed method enables the policy to learn the common features along with the Q function with a neural network. The neural network policy start learning from the target environment and fine tune with the new task. During the process of learning the target task, the policy function would use the previously learned common features to make the learning faster. 
\linebreak
\par 

\bibliographystyle{abbrv}
\bibliography{references}
\end{document}


